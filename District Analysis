---
title: "**District Wide Model Analysis**"
author: '`r params$author`'
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: 
  html_document:
    theme: darkly
    toc: true
    toc_float:
      collapsed: true
params:
  author:
    label: "Analysis Conducted By:"
    value: Ryan Budnik
    input: select
    choices: [Ryan Budnik, Graydon Marzen, Jim Rouse, Ryan Frasch, Shawn Bryant]
  data:
    label: "Input dataset:"
    value: 18CC.csv
    input: select
    choices: [16CC.csv, 17CC.csv, 18CC.csv]
  exp:
    label: "Experiment:"
    value: C81A
    input: select
    choices: [C61A,	C61B,	C62A,	C62B,	C63A,	C63B, C71A,	C71B,	C72A,	C72B,	C73A,	C73B, C81A,	C81B,	C82A, C82B,	C83A,	C83B]
  year:
    label: "Year"
    value: 2018
    input: select
    choices: [2018, 2017, 2016]
  district:
    label: "District:"
    value: Northern
    input: select
    choices: [Northern, Central, Southern]
  season:
    label: "Season Length:"
    value: Early
    input: select
    choices: [Early, Full]
---

```{r logo, cache=TRUE, echo=FALSE}
htmltools::img(src = knitr::image_uri(file.path("C:","Users","rjbudnik","Documents", "DistrictAnalysis","inst","rmarkdown","templates","report","district","ICIA_logo.png")), alt = 'logo', style = 'position:absolute; top:0; right:0; padding:10px;')
```

---

# Overview:
The following report renders a district-wide mixed model analysis with corresponding figures for the *`r params$year` `r params$district` District `r params$season`-season Test*, **`r params$exp`**.

```{r libs, include = FALSE}
library(rmarkdown)
library(knitr)
library(dplyr)
library(agricolae)
library(nlme)
library(lme4)
library(lmerTest)
library(randomcoloR)
library(car)
library(predictmeans)
library(shiny)
library(shinythemes)
library(psych)
library(emmeans)
library(MuMIn)
```

- Corn Coversion Factor = **8.067**
- Bean Conversion Factor = **7.779**

```{r setup, echo = FALSE}
###Set wd & Import CSV Dataset:
df<-read.csv(c(params$data),header=T,na.strings=c("", " ", "NA"))

CCF<-8.067
df<-df%>%mutate(yld = (wt*CCF*(100-moist)/84.5))

###Subset *experiment* specific data of interest:
data<-filter(df, exp==c(params$exp))

###Reduce data to include only neccessary variables:: 
data<-data%>%select("exp", "book.name", "plot", "rep", "entry", "pass", "rng", "yld")

###View Classifications & Set Factors:
data$book.name<-factor(data$book.name)
data$exp<-factor(data$exp)
data$plot<-factor(data$plot)
data$rep<-factor(data$rep)
data$entry<-factor(data$entry)
data$pass<-factor(data$pass)
data$rng<-factor(data$rng)

###Variance, Mean & IQRS:
Entry.Variance<-var(as.numeric(data$entry))
avg.yld <- mean(data$yld, na.rm = TRUE)
max.yld <- quantile(data$yld,0.75, na.rm=TRUE) + (IQR(data$yld, na.rm=TRUE) * 1.5 )
min.yld <- quantile(data$yld,0.25, na.rm=TRUE) - (IQR(data$yld, na.rm=TRUE) * 1.5 )

# Percent function::
percent<-function(x, digits = 6, format = "f", ...) {paste0(formatC( x, format = format, digits = digits, ...),"%")}
```

# Data Visualization: {.tabset .tabset-fade}

## Summary

```{r summary, echo = FALSE, cols.print=15, warning=FALSE}
described<-describe(data, skew=FALSE, ranges=TRUE)
paged_table(described, options(scipen=999, digits=4))
```

## Histograms

```{r histo.yld, echo = FALSE, fig.width=10, fig.height=6.5}
histo<-hist(data$yld, breaks=seq(0,350, l=150), col="purple", main="Distribution of Yield", xlab="Yield")
abline(v = max.yld, col="red")
abline(v = avg.yld, col="red", lwd=c(3))
abline(v = min.yld, col="red")
```

## Barplots

``` {r bar.yld, echo = FALSE, fig.width=10, fig.height=7}
barplot(data$yld, names.arg=data$entry, ylim=c(0,300), main="Distribution of Yield", ylab="Yield", xlab="Entry", cex.names=.4)
abline(h = max.yld, col="red")
abline(h = avg.yld, col="red", lwd=c(3))
abline(h = min.yld, col="red")
```

## Boxplot

``` {r boxplot, comments=NA, echo = FALSE, fig.width=10, fig.height=7, message=FALSE}
Boxplot(data$yld~data$entry,id=TRUE, main="2018 Yield Across Entries", xlab="entry", ylab="Yld", las=2)
abline(h = max.yld, col="red")
abline(h = avg.yld, col="red", lwd=c(3))
abline(h = min.yld, col="red")
```


# Mixed Model Analyses: {.tabset .tabset-fade}

**YIELD = G + E + Rep(E)**

* RANDOM = + Pass(Rep(E)) + Rng(Rep(E)) + G(E)

```{r model1, echo=FALSE}
model1<-lmer(yld~entry + book.name + rep:book.name + (1|pass:book.name:rep) + (1|rng:book.name:rep) + (1|entry:book.name), data=data, REML=TRUE)
```

## Least Square (LS) Means  {.tabset .tabset-fade}

+ Observed Means: Regular arithmetic means that can be computed by hand directly on your data without reference to any statistical model.

+ **Least Squares Means (LS Means):** Means that are computed based on a linear model such as ANOVA.

### By Entry

```{r lsm.by.entry, comment=NA, echo=FALSE, cols.print=8, rows.print=15, message=FALSE}
options(scipen=6)
LSM.Entry<-ls_means(model1, which="entry")
paged_table(LSM.Entry)
```

### By Location

```{r lsm.by.book.name, comment=NA, echo=FALSE, cols.print=8, rows.print=10, message=FALSE}
options(scipen=6)
LSM.District<-ls_means(model1, which="book.name")
paged_table(LSM.District)
```

### Average & Variance

```{r lsm.avg.var, comment=NA, echo=FALSE, message=FALSE}
lsm.entry<-mean(LSM.Entry$Estimate)
lsm.loc<-mean(LSM.District$Estimate)
lsv.entry<-var(LSM.Entry$Estimate)
lsv.loc<-var(LSM.District$Estimate)
Avg.Var.Table<-data.frame("Source"=c("Entry", "Location"), "Average" = c(lsm.entry, lsm.loc), "Variance" = c(lsv.entry, lsv.loc))
paged_table(Avg.Var.Table)
```


## ANOVA Tables

A small *P-value*, and a large *F-statistic* constitute grounds for null hypothesis rejection.

ANOVA uses the *F-statistic* to determine whether the variability between group means is larger than the variability of the observations within the groups. If that ratio is sufficiently large, you can conclude that not all the means are equal. A low *p-value* suggests that your sample provides enough evidence that you can reject the null hypothesis for the entire population.

* The **null hypothesis** states that the model with no independent variables fits the data as well as your model.

* The **alternative hypothesis** says that your model fits the data better than the intercept-only model.


Analysis of Variance Table of **Type I** with Kenward-Roger approximation for degrees of freedom:

```{r modelanova1, cols.print=9, echo = FALSE}
options(scipen=3)
paged_table(anovalmer(model1))
```

**Type III** Analysis of Variance Table with Satterthwaite's method:

```{r modelanova2, echo = FALSE}
options(scipen=3)
paged_table(anova(model1))
```



## Random-Effects Table

```{r model.step.random, echo=FALSE, message=FALSE}
options(scipen=6)
paged_table(ranova(model1))
```


## CV, RMSE & R²

**Coefficient of Vairance (CV):**

The lower the CV, the smaller the residuals relative to the predicted value.  This is suggestive of a good model fit. 

* **SMALLER IS BETTER!** (i.e. The model with the smaller CV has predicted values that are closer to the actual values.)


**Root Mean Square Error(RMSE):**

*RMSE is an absolute measure of fit.*

The standard deviation of the residuals. RMSE physically measures how spread out these residuals are from the line of best fit. The RMSE is a measure of the average deviation of the estimates from the observed values.

* **SMALLER IS BETTER!** (i.e. lower values of RMSE indicate better fit.)


**R-squared (R²):**

*R² is a relative measure of fit.*

R² is the proportional improvement in prediction from the regression model, compared to the mean model. R² indicates the goodness of fit of the model. The fraction of the total sum of squares that is explained by the regression. Scaled from 0 to 1.

* **LARGER IS BETTER!** (i.e. values closer to 1 indicate better fit.)

```{r model1.cv.rmse, comment=NA, echo = FALSE, message=FALSE}
##CV from "LSDer" package:
mod1.cv<-LSDer::CVer(model1)

# ####NOT THE SAME AS SAS::::
# ##LSD from "LSDer" package:
# mod1.lsd.90<-LSDer::LSDer(model1, "entry", level=0.90)
# mod1.lsd.75<-LSDer::LSDer(model1, "entry", level=0.75)
# mod1.lsd.90
# mod1.lsd.75
# 
# # ##"Plantbreeding" package LSD approach:
# vcor<-as.data.frame(VarCorr(model1))
# entryL<-as.numeric(nlevels(data$entry))
# t_alpha<-qt(p=1-(0.25/2), df=(entryL-1))
# V_R<-subset(vcor, grp == "Residual", vcov, drop = TRUE)
# n_rep<-as.numeric(n_distinct(data[,"rep"]))
# LSD.v2<-t_alpha*sqrt(2*(V_R/n_rep))
# LSD.v2

mod1.rmse<-sqrt(mean(residuals(model1)^2))
mod1.R2<-r.squaredLR(model1)
CV.RMSE.R2.Table1<-data.frame("CV" = c(mod1.cv), "RMSE" = c(mod1.rmse), "R²" = c(mod1.R2))
paged_table(CV.RMSE.R2.Table1)
```

## Fisher's LSD

**Fisher's Least Significant Differnce (LSD):**

Least significant difference is used to compare means of different treatments that have an equal number of replications.

* **The yields must be greater than the LSD value between any two treatments to be considered significant.**

``` {r model1.predict.25, comment=NA, echo = FALSE, fig.width=10, fig.height=6.5}
pre1<-predictmeans(model1, "entry", pairwise=TRUE, level=0.25, pplot=FALSE, mplot=FALSE, newwd=FALSE)
LSDpre1<-pre1[["LSD"]][["Aveg.LSD"]]
LSD.Table1<-data.frame("."="Average LSD at 75% Confidence Level =", "VALUE" = as.factor(c(LSDpre1)))
paged_table(LSD.Table1)
pre1.1<-predictmeans(model1, "entry", pairwise=TRUE, level=0.25, pplot=FALSE, mplot=TRUE, newwd=FALSE)
```

```{r model1.lsdplot, echo=FALSE, fig.width=10, fig.height=6.5}
pre1.2<-predictmeans(model1, "entry", pairwise=TRUE, level=0.25, pplot=TRUE, mplot=FALSE, newwd=FALSE)
```


# Outlier Detection: {.tabset .tabset-fade}

## Normal Q-Q Plot

QQ plot (or quantile-quantile plot) draws the correlation between a given sample and the normal distribution. A 45-degree reference line is also plotted. QQ plots are used to visually check the normality of the data.

```{r qq, echo = FALSE, fig.width=9, warning = FALSE}
qq<-qqnorm(resid(model1), pch=23, bg="black")
qqline(resid(model1), col="red", lwd=c(2))
text(qq, rownames(data), cex=0.8, col= "blue4", pos=4, offset = 0.5)
text(qq, label=(data$entry), cex=0.8, col= "red4", pos=4, offset = 1.6)
```

1. Data Points = Residuals for linear mixed model.
2. <span style="color:blue">Blue</span> Labels = **Row** number.
3. <span style="color:red">Red</span> Labels = **Entry** number.

<!-- ## Cook's Distance -->

<!-- Cook's distance is useful for identifying outliers in the X values (observations for predictor variables). It also shows the influence of each observation on the fitted response values. An observation with Cook's distance larger than three times the mean Cook's distance might be an outlier. -->

<!--   + Graphical Reference Thresholds: -->
<!--       1. 10/*n*-*k*-1 -->
<!--       2. 8/*n*-*k*-1 -->
<!--       3. 4/*n*-*k*-1, (**Hair et.al**,1998) -->

<!--       where, -->

<!--       *n* = sample size -->

<!--       *k* = number of independent variables -->

<!-- ```{r cook, echo = FALSE, message=FALSE} -->
<!-- sample_size<-nrow(data) -->
<!-- entryL<-as.numeric(length(data$entry)) -->
<!-- Cooksd<-CookD(model1, newwd=FALSE) -->
<!-- abline(h = (10/(sample_size-entryL-1)), col="red") -->
<!-- abline(h = (8/(sample_size-entryL-1)), col="red", lwd=c(2)) -->
<!-- abline(h = (4/(sample_size-entryL-1)), col="red", lwd=c(3)) -->
<!-- ``` -->

<!-- # References: -->
<!-- 1. **Hair, J., Anderson, R., Tatham, R. and Black W.** (1998). Multivariate Data Analysis (fifth edition). Englewood Cliffs, NJ: Prentice-Hall. -->

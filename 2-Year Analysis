---
title: "**2-Year Analysis**"
author: '`r params$author`'
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: 
  html_document:
    theme: darkly
    toc: true
    toc_float:
      collapsed: true
params:
  author:
    label: "Analysis Conducted By:"
    value: Ryan Budnik
    input: select
    choices: [Ryan Budnik, Graydon Marzen, Jim Rouse, Ryan Frasch, Shawn Bryant]
  data1:
    label: "Current Year's Dataset:"
    value: 18CC.csv
    input: select
    choices: [18CC.csv]
  data2:
    label: "Preivous Year's Dataset:"
    value: 17CC.csv
    input: select
    choices: [17CC.csv]
  exp1:
    label: "Experiment Current Year:"
    value: C81A
    input: select
    choices: [C81A,	C81B,	C82A, C82B,	C83A,	C83B]
  exp2:
    label: "Experiment Previous Year:"
    value: C71A
    input: select
    choices: [C71A,	C71B,	C72A,	C72B,	C73A,	C73B]
  year:
    label: "Year"
    value: 2018
    input: select
    choices: [2018, 2017, 2016]
  district:
    label: "District:"
    value: Northern
    input: select
    choices: [Northern, Central, Southern]
  season:
    label: "Season Length:"
    value: Early
    input: select
    choices: [Early, Full]
---

```{r logo, cache=TRUE, echo=FALSE}
htmltools::img(src = knitr::image_uri(file.path("C:","Users","rjbudnik","Documents", "DistrictAnalysis","inst","rmarkdown","templates","report","district","ICIA_logo.png")), alt = 'logo', style = 'position:absolute; top:0; right:0; padding:10px;')
```

---

# Overview:
The following report renders a 2-Year mixed model analysis with corresponding figures for the *`r params$year` `r params$district` District `r params$season`-season Test*, **`r params$exp1`**.

```{r libs, include = FALSE}
library(rmarkdown)
library(knitr)
library(dplyr)
library(nlme)
library(lme4)
library(lmerTest)
library(randomcoloR)
library(car)
library(predictmeans)
library(shiny)
library(shinythemes)
library(psych)
library(emmeans)
library(MuMIn)
```

- Corn Coversion Factor = **8.067**
- Bean Conversion Factor = **7.779**

# Current Year's Data

```{r Current.Year.Data, echo = FALSE}
## Import Current Years Dataset::
df.C<-read.csv(c(params$data1),header=T,na.strings=c("", " ", "NA"))

## ADD Yield::
CCF<-8.067
df.C<-df.C%>%mutate(yld = (wt*CCF*(100-moist)/84.5))

## Subset *experiment* specific data of interest:
data.C<-filter(df.C, exp==c(params$exp1))

###Reduce data to include only neccessary variables:: 
data.C<-data.C%>%select("book.name","exp","rep","entry","plot","pass","rng","wt","moist","rtldg","skldg","ped.id","code","yld")

## Set Factors:
data.C$book.name<-factor(data.C$book.name)
data.C$exp<-factor(data.C$exp)
data.C$plot<-factor(data.C$plot)
data.C$rep<-factor(data.C$rep)
data.C$entry<-factor(data.C$entry)
data.C$pass<-factor(data.C$pass)
data.C$rng<-factor(data.C$rng)
data.C$code<-factor(data.C$code)

###Variance, Mean & IQRS:
# Entry.Variance<-var(as.numeric(data.C$entry))
# avg.yld <- mean(data.C$yld, na.rm = TRUE)
# max.yld <- quantile(data.C$yld,0.75, na.rm=TRUE) + (IQR(data.C$yld, na.rm=TRUE) * 1.5 )
# min.yld <- quantile(data.C$yld,0.25, na.rm=TRUE) - (IQR(data.C$yld, na.rm=TRUE) * 1.5 )
```

# Previous Year's Data

```{r Previous.Year.Data, echo = FALSE}
## Import Current Years Dataset::
df.P<-read.csv(c(params$data2),header=T,na.strings=c("", " ", "NA"))

## ADD Yield::
CCF<-8.067
df.P<-df.P%>%mutate(yld = (wt*CCF*(100-moist)/84.5))

## Subset *experiment* specific data of interest:
data.P<-filter(df.P, exp==c(params$exp2))

###Reduce data to include only neccessary variables:: 
data.P<-data.P%>%select("book.name","exp","rep","entry","plot","pass","rng","wt","moist","rtldg","skldg","ped.id","code","yld")


## Set Factors:
data.P$book.name<-factor(data.P$book.name)
data.P$exp<-factor(data.P$exp)
data.P$plot<-factor(data.P$plot)
data.P$rep<-factor(data.P$rep)
data.P$entry<-factor(data.P$entry)
data.P$pass<-factor(data.P$pass)
data.P$rng<-factor(data.P$rng)
data.P$code<-factor(data.P$code)

# ###Variance, Mean & IQRS:
# Entry.Variance<-var(as.numeric(data.P$entry))
# avg.yld <- mean(data.P$yld, na.rm = TRUE)
# max.yld <- quantile(data.P$yld,0.75, na.rm=TRUE) + (IQR(data.P$yld, na.rm=TRUE) * 1.5 )
# min.yld <- quantile(data.P$yld,0.25, na.rm=TRUE) - (IQR(data.P$yld, na.rm=TRUE) * 1.5 )
```

# Combine 2-Years

```{r combine.Years, echo=FALSE}
## Combine 2-Year datasets:
data<-rbind((data.C),(data.P))
```


# Data Visualization: {.tabset .tabset-fade}

## Summary

```{r summary, echo = FALSE, cols.print=15, warning=FALSE}
data.small<-data%>%select("code","wt","moist","yld")
described<-describe(data.small, skew=FALSE, ranges=TRUE)
paged_table(described, options(scipen=999, digits=4))
```

## Histograms

```{r histo.yld, echo = FALSE, fig.width=10, fig.height=6.5}
histo<-hist(data$yld, breaks=seq(0,350, l=150), col="purple", main="Distribution of Yield", xlab="Yield")
```

## Barplots

``` {r bar.yld, echo = FALSE, fig.width=10, fig.height=7}
barplot(data$yld, names.arg=data$code, ylim=c(0,300), main="Distribution of Yield", ylab="Yield", xlab="Code", cex.names=.5)
```

## Boxplot

``` {r boxplot, comments=NA, echo = FALSE, fig.width=10, fig.height=7, message=FALSE}
Boxplot(data$yld~data$code,id=TRUE, main="2018 Yield Across Entries", xlab="Code", ylab="Yld", las=2)
```


# Mixed Model Analyses: {.tabset .tabset-fade}

**YIELD = G + E + Rep(E)**

* RANDOM = + Pass(Rep(E)) + Rng(Rep(E)) + G(E)

```{r 2.year.model, comment=NA, echo=FALSE, message=FALSE}
## Linear Mixed Model by 2-Year code
model<-lmer(yld~code + book.name + rep:book.name + (1|pass:book.name:rep) + (1|rng:book.name:rep) + (1|code:book.name), data=data, REML=TRUE)

## Subset codes that appear in previous year:
# data.LSM1<-subset(LSM.Code, code %in% data.C$code)
# data.LSM2<-subset(data.LSM1, code %in% data.P$code)
```

## Least Square (LS) Means

+ Observed Means: Regular arithmetic means that can be computed by hand directly on your data without reference to any statistical model.

+ **Least Squares Means (LS Means):** Means that are computed based on a linear model such as ANOVA.

```{r lsm.by.entry, comment=NA, echo=FALSE, cols.print=8, rows.print=15, message=FALSE}
## LSMs by code
options(scipen=6)
LSM.Code<-as.data.frame(emmeans(model, spec="code"))
LSM.Code<-LSM.Code%>%select(code, emmean)
names(LSM.Code)[2]<-c("ls.yld")

## Print Nicely
paged_table(LSM.Code)
```

### Average & Variance

```{r lsm.avg.var, comment=NA, echo=FALSE, message=FALSE}
## Average and Variance of LSMeans
lsm.entry<-mean(LSM.Code$ls.yld)
lsv.entry<-var(LSM.Code$ls.yld)

## New Table
Avg.Var.Table<-data.frame("Source"=c("Entry"), "Average" = c(lsm.entry), "Variance" = c(lsv.entry))

## Print Nicely
paged_table(Avg.Var.Table)
```

## ANOVA Tables

A small *P-value*, and a large *F-statistic* constitute grounds for null hypothesis rejection.

ANOVA uses the *F-statistic* to determine whether the variability between group means is larger than the variability of the observations within the groups. If that ratio is sufficiently large, you can conclude that not all the means are equal. A low *p-value* suggests that your sample provides enough evidence that you can reject the null hypothesis for the entire population.

* The **null hypothesis** states that the model with no independent variables fits the data as well as your model.

* The **alternative hypothesis** says that your model fits the data better than the intercept-only model.


**Type III** Analysis of Variance Table with Satterthwaite's method:

```{r modelanova2, echo = FALSE}
options(scipen=3)
paged_table(anova(model))
```


## Random-Effects Table

```{r model.step.random, echo=FALSE, message=FALSE}
options(scipen=6)
paged_table(ranova(model))
```


## CV, RMSE & R²

**Coefficient of Vairance (CV):**

The lower the CV, the smaller the residuals relative to the predicted value.  This is suggestive of a good model fit. 

* **SMALLER IS BETTER!** (i.e. The model with the smaller CV has predicted values that are closer to the actual values.)


**Root Mean Square Error(RMSE):**

*RMSE is an absolute measure of fit.*

The standard deviation of the residuals. RMSE physically measures how spread out these residuals are from the line of best fit. The RMSE is a measure of the average deviation of the estimates from the observed values.

* **SMALLER IS BETTER!** (i.e. lower values of RMSE indicate better fit.)


**R-squared (R²):**

*R² is a relative measure of fit.*

R² is the proportional improvement in prediction from the regression model, compared to the mean model. R² indicates the goodness of fit of the model. The fraction of the total sum of squares that is explained by the regression. Scaled from 0 to 1.

* **LARGER IS BETTER!** (i.e. values closer to 1 indicate better fit.)

```{r model1.cv.rmse, comment=NA, echo = FALSE, message=FALSE}
## CV from "LSDer" package:
mod1.cv<-LSDer::CVer(model)
## RMSE
mod1.rmse<-sqrt(mean(residuals(model)^2))
## R-squared
mod1.R2<-r.squaredLR(model)

## New Data Table (CV, RMSE, & R2)
CV.RMSE.R2.Table1<-data.frame("CV" = c(mod1.cv), "RMSE" = c(mod1.rmse), "R²" = c(mod1.R2))
paged_table(CV.RMSE.R2.Table1)
```


## Fisher's LSD

**Fisher's Least Significant Differnce (LSD):**

Least significant difference is used to compare means of different treatments that have an equal number of replications.

* **The yields must be greater than the LSD value between any two treatments to be considered significant.**

``` {r model1.predict.25, comment=NA, echo = FALSE, fig.width=10, fig.height=6.5}
## Average LSD calculation from "predictmeans" package:
pre1<-predictmeans(model, "code", pairwise=TRUE, level=0.25, pplot=FALSE, mplot=FALSE, newwd=FALSE)
## Select Avg. LSD Value from object
LSDpre1<-pre1[["LSD"]][["Aveg.LSD"]]

## New LSD Table
LSD.Table1<-data.frame("."="Average LSD at 75% Confidence Level =", "VALUE" = as.factor(c(LSDpre1)))
paged_table(LSD.Table1)

## MPlot
pre1.1<-predictmeans(model, "code", pairwise=TRUE, level=0.25, pplot=FALSE, mplot=TRUE, newwd=FALSE)
```

```{r model1.lsdplot, echo=FALSE, fig.width=10, fig.height=6.5}
## LSD PPlot
pre1.2<-predictmeans(model, "code", pairwise=TRUE, level=0.25, pplot=TRUE, mplot=FALSE, newwd=FALSE)
```


# Outlier Detection: {.tabset .tabset-fade}

## Normal Q-Q Plot

QQ plot (or quantile-quantile plot) draws the correlation between a given sample and the normal distribution. A 45-degree reference line is also plotted. QQ plots are used to visually check the normality of the data.

```{r qq, echo = FALSE, fig.width=9, warning = FALSE}
qq<-qqnorm(resid(model), pch=23, bg="black")
qqline(resid(model), col="red", lwd=c(2))
text(qq, rownames(data), cex=0.8, col= "blue4", pos=4, offset = 0.5)
text(qq, label=(data$code), cex=0.8, col= "red4", pos=4, offset = 1.6)
```

1. Data Points = Residuals for linear mixed model.
2. <span style="color:blue">Blue</span> Labels = **Row** number.
3. <span style="color:red">Red</span> Labels = **Entry** number.
